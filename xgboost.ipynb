{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "621eda69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79b267bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFiles(path):\n",
    "    files = os.listdir(path)\n",
    "    size = len(files)\n",
    "    return [files, size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0795f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(dataX, dataY, param):\n",
    "    #xGboost training\n",
    "    dtrain = xgb.DMatrix(dataX, label=dataY)\n",
    "\n",
    "    num_round = 10\n",
    "    bst = xgb.train(param, dtrain, num_round)\n",
    "    \n",
    "    return bst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "090f0e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, dataX, labels):\n",
    "    pre = model.predict(xgb.DMatrix(dataX))\n",
    "    j = 0\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == pre[i]:\n",
    "            j+=1\n",
    "    return 100 * j / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98f0e17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(dataX, dataY, validation_train_index, validation_test_index, num_class):\n",
    "    param = dict()\n",
    "    MAX_ACC = 0\n",
    "    best_params = None\n",
    "    \n",
    "    numOffold_inner = validation_train_index.shape[1]\n",
    "    \n",
    "    trainAcc = np.zeros(numOffold_inner)\n",
    "    testAcc = np.zeros(numOffold_inner)\n",
    "    \n",
    "    trainAcc_best = 0\n",
    "    testAcc_best = 0\n",
    "    \n",
    "    for eta in options[\"eta\"]:\n",
    "        for gamma in options[\"gamma\"]:\n",
    "            for max_depth in options[\"max_depth\"]:\n",
    "                for min_child_weight in options[\"min_child_weight\"]:\n",
    "                    for subsample in options[\"subsample\"]:\n",
    "                        param[\"eta\"] = eta\n",
    "                        param[\"gamma\"] = gamma\n",
    "                        param[\"max_depth\"] = max_depth\n",
    "                        param[\"min_child_weight\"] = min_child_weight\n",
    "                        param[\"subsample\"] = subsample\n",
    "                        param['nthread'] = 4\n",
    "                        param['eval_metric'] = 'auc'\n",
    "                        param['num_class'] = num_class\n",
    "                        \n",
    "                        #Inner Cross Validation\n",
    "                        for num_CV_inner in range(numOffold_inner):\n",
    "                            dataX_train = dataX[validation_train_index[:, num_CV_inner]]\n",
    "                            dataY_train = dataY[validation_train_index[:, num_CV_inner]]\n",
    "                            dataX_test = dataX[validation_test_index[:, num_CV_inner]]\n",
    "                            dataY_test = dataY[validation_test_index[:, num_CV_inner]]\n",
    "                            \n",
    "                            bst = model(dataX_train, dataY_train, param)\n",
    "                            \n",
    "                            trainAcc[num_CV_inner] = accuracy(bst, dataX_train, dataY_train)\n",
    "                            testAcc[num_CV_inner] = accuracy(bst, dataX_test, dataY_test)\n",
    "\n",
    "                        TMP_ACC = np.mean(testAcc)\n",
    "\n",
    "                        if TMP_ACC > MAX_ACC:\n",
    "                            MAX_ACC = TMP_ACC\n",
    "                            best_params = param\n",
    "                            trainAcc_best = np.mean(trainAcc)\n",
    "                            testAcc_best = np.mean(testAcc)\n",
    "    return [best_params, trainAcc_best, testAcc_best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a82cd386",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'xgboost'\n",
    "dataset_path = \"D:/project-btp/codes/Shared_Datasets/\"\n",
    "path1 = \"D:\\\\project-btp\\\\results\\\\\" + method + \"\\\\\"\n",
    "\n",
    "if not os.path.isdir(path1):\n",
    "    os.mkdir(path1)\n",
    "\n",
    "[name, num] = getFiles(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91ff04b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abalone\n",
      "1 th fold - trainAcc:  87.99872326843281 testAcc:  64.55938697318008\n",
      "2 th fold - trainAcc:  88.66900734120651 testAcc:  63.69731800766284\n",
      "3 th fold - trainAcc:  88.73284391956591 testAcc:  62.93103448275862\n",
      "4 th fold - trainAcc:  88.19023300351101 testAcc:  65.51724137931035\n",
      "Final Training data accuracy:  88.39770188317905\n",
      "Final Testing data accuracy:  64.17624521072797\n",
      "adult\n",
      "1 th fold - trainAcc:  88.98989588771843 testAcc:  86.46889011731466\n",
      "Final Training data accuracy:  88.98989588771843\n",
      "Final Testing data accuracy:  86.46889011731466\n",
      "car\n",
      "1 th fold - trainAcc:  98.8425925925926 testAcc:  97.91666666666667\n",
      "2 th fold - trainAcc:  98.99691358024691 testAcc:  96.99074074074075\n",
      "3 th fold - trainAcc:  99.45987654320987 testAcc:  97.68518518518519\n",
      "4 th fold - trainAcc:  98.99691358024691 testAcc:  97.45370370370371\n",
      "Final Training data accuracy:  99.07407407407408\n",
      "Final Testing data accuracy:  97.51157407407408\n",
      "iris\n",
      "1 th fold - trainAcc:  97.34513274336283 testAcc:  97.29729729729729\n",
      "2 th fold - trainAcc:  96.46017699115045 testAcc:  94.5945945945946\n",
      "3 th fold - trainAcc:  96.46017699115045 testAcc:  94.5945945945946\n",
      "4 th fold - trainAcc:  98.23008849557522 testAcc:  97.29729729729729\n",
      "Final Training data accuracy:  97.12389380530973\n",
      "Final Testing data accuracy:  95.94594594594594\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "booster: gbtree, gblinear, dart\n",
    "eta: [0;1, 0.1 stp]\n",
    "gamma = [0:0.5, 0.1 step]\n",
    "max_depth = [ 3, 4, 5, 6, 8, 10, 12, 15]\n",
    "\"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    "subsample = [0:1, 0.5]\n",
    "objective is also decidable\n",
    "\"\"\"\n",
    "options = {\n",
    "    \"eta\" : np.arange(0.1, 0.4, 0.1),\n",
    "    \"gamma\" : np.arange(0.1, 0.4, 0.1),\n",
    "    \"max_depth\" : [ 5, 10],\n",
    "    \"min_child_weight\" : [ 1, 5 ],\n",
    "    \"subsample\" : [0.5, 1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(columns = [\"dataset_name\", \"training_accuracy\", \"testing_accuracy\"])\n",
    "\n",
    "for dataset_num in range(num):\n",
    "    dataset_name = name[dataset_num]\n",
    "    print(dataset_name)\n",
    "    \n",
    "    #Check for repitition\n",
    "    \"\"\"to do\"\"\"\n",
    "    \n",
    "    dataset_path_name = dataset_path + dataset_name + '\\\\'\n",
    "    \n",
    "    #loading dataset\n",
    "    folds = None\n",
    "    labels = None\n",
    "    validation_train = None\n",
    "    validation_test = None\n",
    "    \n",
    "    with h5py.File(dataset_path_name + \"folds.mat\", \"r\") as f:\n",
    "        folds = np.array(f.get(\"folds\")).T\n",
    "    with h5py.File(dataset_path_name + \"labels.mat\", \"r\") as f:\n",
    "        labels = np.array(f.get(\"labels\")).T\n",
    "    with h5py.File(dataset_path_name + \"validation_train.mat\", \"r\") as f:\n",
    "        validation_train = np.array(f.get(\"validation_train\")).T\n",
    "    with h5py.File(dataset_path_name + \"validation_test.mat\", \"r\") as f:\n",
    "        validation_test = np.array(f.get(\"validation_test\")).T\n",
    "    \n",
    "    numOffold = None\n",
    "    try:\n",
    "        with h5py.File(dataset_path_name + \"numOffold.mat\", \"r\") as f:\n",
    "            numOffold = int(np.array(f.get(\"numOffold\")).item())\n",
    "    except:\n",
    "        numOffold = folds.shape[1] #4-fold CV\n",
    "    \n",
    "    with h5py.File(dataset_path_name + dataset_name + \".mat\", \"r\") as f:\n",
    "        dataX = np.array(f.get(dataset_name)).T\n",
    "    dataY = labels\n",
    "    \n",
    "    folds = (folds != 0)\n",
    "    \n",
    "    validation_test = (validation_test != 0)\n",
    "    validation_train = (validation_train != 0)\n",
    "    \n",
    "    U_dataY = np.unique(dataY)\n",
    "    nclass = len(U_dataY)\n",
    "    \n",
    "    \n",
    "    #results which will be saved\n",
    "    trainAcc = [0] * numOffold\n",
    "    testAcc = [0] * numOffold\n",
    "    \n",
    "    options_saved = [0] * numOffold\n",
    "    val_trainAcc_All = [0] * numOffold\n",
    "    val_testAcc_All = [0] * numOffold\n",
    "    \n",
    "    #Outer Cross-validation\n",
    "    for num_CV in range(numOffold):\n",
    "        print(num_CV+1, \"th fold - \", end = \"\")\n",
    "        \n",
    "        test_index = folds[:, num_CV]\n",
    "        train_index = [not x for x in test_index]\n",
    "        \n",
    "        validation_train_index = validation_train[:, num_CV * numOffold: (num_CV + 1) * numOffold]\n",
    "        validation_test_index = validation_test[:, num_CV * numOffold: (num_CV + 1) * numOffold]\n",
    "        \n",
    "        [params, val_trainAcc, val_testAcc] = grid_search(dataX, dataY, validation_train_index, validation_test_index, nclass)\n",
    "        \n",
    "        bst = model(dataX[train_index], dataY[train_index], params)\n",
    "        \n",
    "        #saving results\n",
    "        val_trainAcc_All[num_CV] = val_trainAcc\n",
    "        val_testAcc_All[num_CV] = val_testAcc\n",
    "        options_saved[num_CV] = params\n",
    "        trainAcc[num_CV] = accuracy(bst, dataX[train_index], labels[train_index])\n",
    "        testAcc[num_CV] = accuracy(bst, dataX[test_index], labels[test_index])\n",
    "        print(\"trainAcc: \", trainAcc[num_CV], \"testAcc: \", testAcc[num_CV])\n",
    "    \n",
    "    print(\"Final Training data accuracy: \", np.mean(trainAcc))\n",
    "    print(\"Final Testing data accuracy: \", np.mean(testAcc))\n",
    "\n",
    "    results = {\n",
    "        \"validation_trainAcc\" : val_trainAcc_All,\n",
    "        \"validation_testAcc\" : val_testAcc_All,\n",
    "        \"options_saved\" : options_saved,\n",
    "        \"trainAcc\" : trainAcc,\n",
    "        \"testAcc\" : testAcc\n",
    "    }\n",
    "    \n",
    "    #Saving results in json\n",
    "    filename = path1 + \"Res_\" + dataset_name + \".json\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(results, f)\n",
    "    \n",
    "    df = df.append({\"dataset_name\" : dataset_name,\n",
    "               \"training_accuracy\" : round(np.mean(trainAcc), 2),\n",
    "               \"testing_accuracy\" :  round(np.mean(testAcc), 2)},\n",
    "              ignore_index = True)\n",
    "\n",
    "#Final results in Excel Sheet\n",
    "df.to_excel(path1 + \"fixed_all_results.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4186b138",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., ..., 1., 1., 2.])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \"\"\"\n",
    "     Commments for sanity check\n",
    "     To do or not to do\n",
    "     \n",
    "    #0-1 encoding for target\n",
    "    for i in range(nclass):\n",
    "        idx = (dataY==U_dataY[i])\n",
    "        for j in range(len(idx)):\n",
    "            if idx[j]:\n",
    "                dataY_temp[j][i] = 1\n",
    "    \n",
    "    dataset_path = \"D:\\\\project-btp\\\\codes\\\\data_csv\\\\\"\n",
    "    path1 = \"D:\\\\project-btp\\\\\" + method + \"\\\\\"\n",
    "    \n",
    "    folds = np.genfromtxt(dataset_path_name + 'folds.csv', delimiter=',')\n",
    "    labels = np.genfromtxt(dataset_path_name + 'labels.csv', delimiter=',')\n",
    "    validation_train = np.genfromtxt(dataset_path_name + 'validation_train.csv', delimiter=',')\n",
    "    validation_test = np.genfromtxt(dataset_path_name + 'validation_test.csv', delimiter=',')\n",
    "    numOffold = np.genfromtxt(dataset_path_name + 'numOffolds.csv', delimiter=',')\n",
    "    dataX = np.genfromtxt(dataset_path_name + dataset_name + \".csv\", delimiter=',')\n",
    "    \n",
    "    eta = np.arange(0.1,    gamma = np.arange(0, 0.3, 0.1)\n",
    "    max_depth = [ 5, 10]\n",
    "    min_child_weight = [ 1, 5 ]\n",
    "    subsample = [0.5, 1]\n",
    "    i = 0\n",
    "    for e in eta:\n",
    "        for g in gamma:\n",
    "            for depth in max_depth:\n",
    "                for mcw in min_child_weight:\n",
    "                    for ss in subsample:\n",
    "                        i+=1 0.3, 0.1)\n",
    "\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
